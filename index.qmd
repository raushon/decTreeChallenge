---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding

## Challenge Overview

**Your Mission:** Create a simple GitHub Pages site that demonstrates how decision trees measure feature importance and analyzes the critical differences between categorical and numerical variable encoding. You'll answer two key discussion questions by adding narrative to a pre-built analysis and posting those answers to your GitHub Pages site as a rendered HTML document.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Decision Tree Problem üéØ

> "The most important thing in communication is hearing what isn't said." - Peter Drucker

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

::: {.callout-important}
## üéØ The Key Insight: Encoding Matters for Interpretability

**The problem:** When we encode categorical variables as numerical values (like 1, 2, 3, 4...), decision trees treat them as if they have a meaningful numerical order. This can completely distort our analysis.

**The Real-World Context:** In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.

**The Devastating Reality:** Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don't properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.

:::

Let's assume we want to predict house prices and understand which features matter most. The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

## The Ames Housing Dataset üè†

We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).

## The Problem: ZipCode as Numerical vs Categorical

**Key Question:** What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

**The Issue:** Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices.  Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Data Loading and Model Building

### Python

```{python}
#| label: load-and-model-python
#| echo: true

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
tree_model = DecisionTreeRegressor(max_depth=3, 
                                  min_samples_split=20, 
                                  min_samples_leaf=10, 
                                  random_state=123)
tree_model.fit(X_train, y_train)

print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```

## Tree Visualization

### Python

```{python}
#| label: visualize-tree-python
#| echo: true
#| fig-width: 10
#| fig-height: 6

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(tree_model, 
          feature_names=X_train.columns,
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)
plt.title("Decision Tree (zipCode as Numerical)")
plt.tight_layout()
plt.show()
```

:::

## Feature Importance Analysis

### Python

```{python}
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': tree_model.feature_importances_
}).sort_values('Importance', ascending=False)

importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

```{python}
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_df)), importance_df['Importance'], 
         color='steelblue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode as Numerical)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

## Critical Analysis: The Encoding Problem

::: {.callout-warning}
## ‚ö†Ô∏è The Problem Revealed

**What to note:** Our decision tree treated `zipCode` as a numerical variable.  This leads to zip code being unimportant.  Not surprisingly, because there is no reason to believe allowing splits like "zipCode < 50012.5" should be beneficial for house price prediction. This false coding of a variable creates several problems:

1. **Potentially Meaningless Splits:** A zip code of 50013 is not "greater than" 50012 in any meaningful way for house prices
2. **False Importance:** The algorithm assigns importance to zipCode based on numerical splits rather than categorical distinctions OR the importance of zip code is completely missed as numerical ordering has no inherent relationship to house prices.
3. **Misleading Interpretations:** We might conclude zipCode is not important when our intuition tells us it should be important (listen to your intuition).

**The Real Issue:** Zip codes are categorical variables representing discrete geographic areas. The numerical values have no inherent order or magnitude relationship to house prices.  These must be modelled as categorical variables.
:::

## Proper Categorical Encoding: The Solution

Now let's repeat the analysis with zipCode properly encoded as categorical variables to see the difference.

**Python Approach:** One-hot encode zipCode (create dummy variables for each zip code)

### Categorical Encoding Analysis

### Python

```{python}
#| label: categorical-python
#| echo: true
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```

### Tree Visualization: Categorical zipCode

### Python

```{python}
#| label: visualize-tree-cat-python
#| echo: true
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
plt.figure(figsize=(10, 6))
plot_tree(tree_model_cat, 
          feature_names=X_train_cat.columns,
          filled=True, 
          rounded=True,
          fontsize=8,
          max_depth=4)
plt.title("Decision Tree (zipCode One-Hot Encoded)")
plt.tight_layout()
plt.show()
```

### Feature Importance: Categorical zipCode

### Python
```{python}
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_cat_df)), importance_cat_df['Importance'], 
         color='darkgreen', alpha=0.7)
plt.yticks(range(len(importance_cat_df)), importance_cat_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode One-Hot Encoded)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

## Discussion Questions for Challenge

**Your Task:** Add thoughtful narrative answers to these two questions in the Discussion Questions section of your rendered HTML site.

1. **Numerical vs Categorical Encoding:** There are two models in Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?  Is zipcode and ordinal or non-ordinal variable?

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case.  What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html) suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.

3. **Are There Any Suggestions for Implementing Decision Trees in Python With Prioper Categorical Handling?** Please poke around the Internet (AI is not as helpful with new libraries) for suggestions on how to implement decision trees in Python with better (i.e. not one-hot encoding) categorical handling.  Please provide a link to the source and a quote from the source.  There is not right answer here, but please provide a thoughtful answer, I am curious to see what you find.

## Answers to Discussion Questions

### 1. Numerical vs Categorical Encoding: How Should Zip Code Be Modeled?

Zip codes should be treated as **categorical variables**‚Äîspecifically **non-ordinal categories**.

**Why Not Treat Them as Numbers?**

Zip codes look like numbers, but they don‚Äôt behave like numerical values. Each one is just a label for a specific area. The number itself doesn‚Äôt tell you anything meaningful about house prices.

If a decision tree treats zip codes as numeric, it creates splits like ‚ÄúzipCode > 50012.5,‚Äù which make no real-world sense because:

- A zip code of 50013 isn‚Äôt ‚Äúgreater‚Äù than 50012 in any useful way.
- The numeric order of zip codes doesn‚Äôt match neighborhood quality or housing trends.
- Consecutive zip codes can represent completely different areas with very different price levels.

**Why Non-Ordinal?**

Zip codes have no natural order. Unlike true ordinal categories (like ‚Äúpoor ‚Üí fair ‚Üí good ‚Üí excellent‚Äù), zip codes can‚Äôt be ranked in a meaningful way. A zip code like 99999 isn‚Äôt ‚Äúbetter‚Äù or ‚Äúmore expensive‚Äù than 10001‚Äîit‚Äôs simply a different location.

**Why This Matters**

Location is a major driver of home prices, and different zip codes can vary widely in school quality, crime rates, amenities, and overall demand. None of these differences are reflected in the numeric sequence of the zip codes.

By treating zip codes as categorical variables (using one-hot encoding or similar methods), decision trees can correctly learn which specific areas are associated with higher or lower prices‚Äîinstead of trying to find meaningless numeric cutoffs.

**In Short**

Modeling zip codes as categorical, non-ordinal variables helps decision trees capture real differences between neighborhoods. Treating them as numbers hides important location information and can lead to misleading results.

### 2. R vs Python Implementation Differences

When modeling zip codes as categorical variables, R and Python produce significantly different decision trees and feature importance rankings due to fundamental differences in how each language's libraries handle categorical data.

**The Core Difference: Native Support vs. Workarounds**

R's `rpart` package natively supports categorical variables. When you provide a categorical variable (stored as a factor in R), the algorithm can directly split on individual categories or groups of categories during tree construction. This means the decision tree can consider all categories simultaneously and make splits that are meaningful for the categorical structure of the data.

Python's `scikit-learn` library, in contrast, does not natively support categorical variables. The official scikit-learn documentation explicitly acknowledges this limitation. According to the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html):

> "scikit-learn uses an optimized version of the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now."

This limitation means that categorical variables must be preprocessed into numerical formats before they can be used in a decision tree. The most common approach is one-hot encoding, which converts each category into a separate binary feature (e.g., zipCode_50010, zipCode_50011, zipCode_50012, etc.).

**Which Language Does a Better Job?**

R does a better job of modeling zip codes as categorical variables for several reasons.For real estate modeling, where location (zip code) is often one of the most important predictors, R's native categorical support provides a significant advantage. The model can directly identify which zip codes are associated with higher or lower prices without the complexity and potential information loss introduced by one-hot encoding in Python.

**Conclusion**

While both R and Python can model zip codes in decision trees, R's native support for categorical variables makes it the superior choice for this specific task. Python's requirement for one-hot encoding introduces unnecessary complexity, reduces interpretability, and fragments feature importance across multiple binary features. The scikit-learn documentation's explicit acknowledgment that "the scikit-learn implementation does not support categorical variables for now" highlights this as a known limitation that Python users must work around.

### 3. Suggestions for Better Categorical Handling in Python

While scikit-learn's decision trees require one-hot encoding for categorical variables, several modern Python libraries and approaches offer better alternatives that handle categorical data natively. Here are the most promising options:

**1. CatBoost: Native Categorical Support**

CatBoost (Categorical Boosting) is a gradient boosting library developed by Yandex that was specifically designed to handle categorical features efficiently. Unlike scikit-learn, CatBoost can process categorical variables directly without requiring one-hot encoding. The library automatically detects and handles categorical features, using sophisticated techniques like target statistics and ordered boosting to process them efficiently.

To use CatBoost, you simply specify which features are categorical using the `cat_features` parameter during model initialization. This eliminates the dimensionality explosion problem associated with one-hot encoding and allows the model to consider categorical relationships more naturally.

**2. LightGBM: Efficient Categorical Handling**

LightGBM (Light Gradient Boosting Machine), developed by Microsoft, is another gradient boosting framework that natively supports categorical features. Like CatBoost, LightGBM can handle categorical variables directly by specifying them during model training, avoiding the need for one-hot encoding.

As noted in discussions on the DeepLearning.AI community, tree-based algorithms like LightGBM do not require one-hot encoding because:

> "Many Tree based algorithms, like LGBM, do NOT require one-hot encoding. This is because one-hot encoding forces to select only ONE of the values of a categorical variable at a time." ([community.deeplearning.ai](https://community.deeplearning.ai/t/isnt-it-a-bad-idea-to-use-one-hot-encode-for-decision-tree-models/165559))

This capability makes LightGBM particularly well-suited for datasets with high-cardinality categorical variables like zip codes, where one-hot encoding would create hundreds or thousands of binary features.

**3. H2O: Automatic Categorical Processing**

H2O is an open-source machine learning platform that includes decision tree algorithms with native categorical variable support. The H2O Python package automatically handles categorical values efficiently without requiring preprocessing.

As highlighted in a Stack Overflow discussion:

> "The h2o package handles categorical values automatically efficiently. It is recommended to not one-hot-encode them first." ([stackoverflow.com](https://stackoverflow.com/questions/56907349/is-there-a-way-to-use-decision-trees-with-categorical-variables-without-one-hot))

H2O's automatic handling of categorical variables makes it a straightforward alternative for practitioners who want to avoid the preprocessing complexity of one-hot encoding.

**4. Custom CART Implementation: Understanding the Algorithm**

For those interested in understanding how categorical variables can be handled at the algorithm level, there are resources that explain how to modify the CART algorithm itself to process categorical features directly. An article on Inside Learning Machines provides insights into this approach:

> "The CART algorithm makes it possible to handle categorical features within the Decision Tree itself. We can therefore skip the additional step during preprocessing." ([insidelearningmachines.com](https://insidelearningmachines.com/decision_trees_handle_categorical_features/))

This approach involves modifying the tree-building process to allow splits on subsets of categories (e.g., "if zipCode is in {50010, 50012, 50014}") rather than requiring numerical thresholds. While implementing a custom solution requires deeper algorithmic understanding, it provides full control over how categorical variables are processed.

**The Future of Categorical Support in scikit-learn**

While scikit-learn's decision trees currently require one-hot encoding, the machine learning community has clearly recognized this limitation. The development of libraries like CatBoost and LightGBM, which were built from the ground up to handle categorical variables efficiently, demonstrates that native categorical support is not only possible but can lead to better model performance and interpretability. As Python's machine learning ecosystem continues to evolve, we may see scikit-learn add native categorical support in future versions, but for now, these alternative libraries provide the best path forward for practitioners working with categorical data.

::: {.callout-note}
Personal Learning: This is a good example of the importance of understanding the underlying principles of the algorithms you are using.  It is also a good example of the importance of understanding the data you are working with.  In this case, zip codes are categorical variables that should be treated as such.
:::